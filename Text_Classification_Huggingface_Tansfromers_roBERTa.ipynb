{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF based Transformer Embedding Aggregation from word to sentence level for MultiLabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we will develop sentence level embedding based on Huggingface Transformers BERT pre-trained model. We will provide two types of aggregation methods from word level to sentence level, one by regular averaging, the other by weighted average by IDF scores for each token.\n",
    "We also generate a set of embeddings using Wiki Plot Movie datasets, and a demo with multiclass classification example.\n",
    "\n",
    "\n",
    "#### Flow of the notebook\n",
    "\n",
    "The notebook has the following sections:\n",
    "\n",
    "1. [Importing Python Libraries](#section01)\n",
    "2. [Defining modules](#section02)\n",
    "3. [Loading and processing the Dataset](#section03)\n",
    "4. [Generating sentence level embedding: two sets with diff agg methods](#section04)\n",
    "5. [Building Classification Models](#section05)\n",
    "6. [Evaluating Classification Models](#section06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, gc, re, random\n",
    "from collections import defaultdict, Counter\n",
    "import copy\n",
    "import pickle\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Starting with the CountVectorizer/TfidfTransformer approach...\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processWikiPlotData(df):\n",
    "    \n",
    "    proc_df = df[(df[\"Origin/Ethnicity\"]==\"American\") | (df[\"Origin/Ethnicity\"]==\"British\")]\n",
    "    proc_df = proc_df[[\"Plot\", \"Genre\"]]\n",
    "    drop_indices = proc_df[proc_df[\"Genre\"] == \"unknown\" ].index\n",
    "    proc_df.drop(drop_indices, inplace=True)\n",
    "    # Combine genres: 1) \"sci-fi\" with \"science fiction\" &  2) \"romantic comedy\" with \"romance\"\n",
    "    proc_df[\"Genre\"].replace({\"sci-fi\": \"science fiction\", \"romantic comedy\": \"romance\"}, inplace=True)\n",
    "\n",
    "    # Choosing movie genres based on their frequency\n",
    "    shortlisted_genres = proc_df[\"Genre\"].value_counts().reset_index(name=\"count\").query(\"count > 200\")[\"index\"].tolist()\n",
    "    proc_df = proc_df[proc_df[\"Genre\"].isin(shortlisted_genres)].reset_index(drop=True)\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    proc_df = proc_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    proc_df[\"genre_encoded\"] = label_encoder.fit_transform(proc_df[\"Genre\"].tolist())\n",
    "\n",
    "    proc_df = proc_df[[\"Plot\", \"Genre\", \"genre_encoded\"]]\n",
    "    return proc_df\n",
    "\n",
    "def partitionList(sentList, n):\n",
    "    if n == 0:\n",
    "        return [sentList[:]]\n",
    "    chunkSize = int(len(sentList)/n)\n",
    "    remainder = len(sentList) % n\n",
    "    currentIndex = 0\n",
    "    res = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if i < remainder:\n",
    "            res.append(sentList[currentIndex: currentIndex+chunkSize+1])\n",
    "            currentIndex = currentIndex+chunkSize+1\n",
    "        else:\n",
    "            res.append(sentList[currentIndex: currentIndex+chunkSize])\n",
    "            currentIndex = currentIndex+chunkSize\n",
    "    return res\n",
    "\n",
    "def genInputs4preBERT(df, textCol='Plot', labelCol='genre_encoded', chunkSize=1000):\n",
    "    sentList = []\n",
    "    labelList = []\n",
    "    for indx, row in df.iterrows():\n",
    "        sentList.append(row[textCol])\n",
    "        labelList.append(row[labelCol])\n",
    "    return sentList, partitionList(sentList, int(len(sentList)/chunkSize)), labelList\n",
    "\n",
    "def flatenListOfLists(ListOfLists, NumIters=3):\n",
    "    \"\"\"\n",
    "    Flatten the list of list by two stage to guarantee performance on databricks\n",
    "    :param ListOfLists: Input\n",
    "    :param NumIters: number of iterations, exponential growing on chunk size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if NumIters <= 0:\n",
    "        raise ValueError('Iteration number should be at least 1')\n",
    "    chunkSize = int(len(ListOfLists) ** (1/NumIters)) + 1\n",
    "    print ('Chunk size={}'.format(chunkSize))\n",
    "    currList = copy.deepcopy(ListOfLists)\n",
    "\n",
    "    for _ in range(NumIters-1):\n",
    "        tempList = []\n",
    "        loc = 0\n",
    "        print ('Current length of the list={}'.format(len(currList)))\n",
    "        while loc < len(currList):\n",
    "            tempList.append(list(functools.reduce(lambda a, b: a+b, currList[loc:loc+chunkSize])))\n",
    "            loc += chunkSize\n",
    "        currList = copy.deepcopy(tempList)\n",
    "\n",
    "    return list(functools.reduce(lambda a, b: a+b, currList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genIdfDict4BERTToks(tokenizer, sentList, saveObj = True):\n",
    "    \"\"\"\n",
    "    Map from BERT token to IDF score\n",
    "        - IDF scores by Sklearn\n",
    "    Create a dictionary mapping each token in the text corpus\n",
    "        to the idf score \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Get a dictionary of desired token to its IDF score\n",
    "    idfDict = defaultdict(float)\n",
    "\n",
    "    tokensList = list(map(lambda line: \" \".join(tokenizer.tokenize(line, tokens = tokenizer.tokenize(sentence, do_lower_case=False))), sentList))\n",
    "    tokenSet = set(flatenListOfLists(tokensList))    \n",
    "  \n",
    "    tfIdfVectorizer = TfidfVectorizer(stop_words=None, use_idf=True)\n",
    "    T = tfIdfVectorizer.fit_transform(tokensList)\n",
    "    tfidf_features = tfIdfVectorizer.get_feature_names()\n",
    "\n",
    "    # Get a dictionary of desired token to its IDF score\n",
    "    idfDict = defaultdict(float)\n",
    "    start = time()\n",
    "    L = len(tokensList)\n",
    "    print ('tokSet count={}'.format(L))\n",
    "    for i, toks in enumerate(tokensList):\n",
    "        if i % 500 == 0:\n",
    "            print ('Indx={} in {}s out of {}'.format(i, time() - start, L))\n",
    "\n",
    "        tokCounter = Counter(toks.split(' '))\n",
    "        #print(tokCounter)\n",
    "        \n",
    "        numValids = np.sum([tokCounter[tok] for tok in tokCounter if tok in tfidf_features])\n",
    "        for tok in tokCounter:\n",
    "            c = tokCounter[tok]\n",
    "            #print (c)\n",
    "            if tok in idfDict or c == 0:\n",
    "                continue\n",
    "\n",
    "            #print(tok)\n",
    "            try:\n",
    "                currIDF = T[i, tfidf_features.index(tok)] * numValids / c\n",
    "                idfDict[tok] = currIDF\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if saveObj:\n",
    "        fName = \"./dicts/idfDict.pkl\"\n",
    "        with open(fName, 'wb') as f:\n",
    "            pickle.dump(\n",
    "                idfDict,\n",
    "                f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggHFTembByMaskV2(embeddings, masks):\n",
    "    \"\"\"\n",
    "    Function to aggregate word embedding to sentence embedding\n",
    "        for Hugging Face Transformer\n",
    "    :param embeddings: Embedding tensors, multi dimension(word-level) for\n",
    "        each sentence\n",
    "    :param masks: mask tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.to('cpu').numpy()\n",
    "    masks = masks.to('cpu').numpy()\n",
    "    print(list(zip(embeddings, masks))[0][0].shape)\n",
    "    #print(list(zip(embeddings, masks))[0])\n",
    "\n",
    "    maskedVectors = [e[1:np.sum(m)-1] for e, m in zip(embeddings, masks)]\n",
    "\n",
    "    #print([(len(x), len(x[0])) for x in maskedVectors])\n",
    "    return [np.mean(vecs, axis=0) for vecs in maskedVectors]\n",
    "\n",
    "def aggHFTembByIDF(embeddings, weights):\n",
    "    \"\"\"\n",
    "    Function to aggregate word embedding to sentence embedding\n",
    "        for Hugging Face Transformer\n",
    "    :param embeddings: Embedding tensors, multi dimension(word-level) for\n",
    "        each sentence\n",
    "    :param masks: mask tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print (weights)\n",
    "    embeddings = embeddings.to('cpu').numpy()\n",
    "    #print (embeddings.shape, np.array(weights).shape)\n",
    "    maskedVectors = [(e, m) for e, m in zip(embeddings, np.array(weights))]\n",
    "    #print([(len(x), len(x[0])) for x in maskedVectors])\n",
    "\n",
    "    return [np.average(vecs, axis=0, weights=weights) for vecs, weights in maskedVectors]\n",
    "\n",
    "def encodeHFTwWeightByIDF(\n",
    "        tokenizer, model, sentences, idfDict, batch_size=512, maxLen=512,\n",
    "        show_progress_bar=None, device=torch.device('cuda'),\n",
    "        is_parallel=True):\n",
    "    \"\"\"\n",
    "    Pretraine HFT BERT embedding\n",
    "    Agg BERT embedding from token level to sentence level\n",
    "        - Regular average vector and weighted average by token IDF scores\n",
    "    Computes sentence embeddings based on pretrained BERT model:\n",
    "        tokenize with max length\n",
    "        Default model is roBERTa base\n",
    "    :param tokenizer:\n",
    "       Hugging Face tokenizer\n",
    "    :param model:\n",
    "       Hugging Face model\n",
    "    :param sentences:\n",
    "       the sentences to embed\n",
    "    :param batch_size:\n",
    "       batch_size for embedding\n",
    "    :param maxLen:\n",
    "       max Sentence length including [CLS] [SEP]\n",
    "    :param batch_size:\n",
    "       the batch size used for the computation\n",
    "    :param show_progress_bar:\n",
    "        Output a progress bar when encode sentences\n",
    "    :param device:\n",
    "       GPU or CPU\n",
    "    :param is_parallel:\n",
    "       GPU required for is_parallel==True\n",
    "    :return:\n",
    "       a list with ndarrays of the embeddings for each sentence\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    if show_progress_bar is None:\n",
    "        show_progress_bar = (\n",
    "                logging.getLogger().getEffectiveLevel() == logging.INFO or\n",
    "                logging.getLogger().getEffectiveLevel() == logging.DEBUG)\n",
    "\n",
    "    all_embeddingsM = []\n",
    "    all_embeddingsW = []\n",
    "    length_sorted_idx = np.argsort([len(sen.split()) for sen in sentences])\n",
    "\n",
    "    iterator = range(0, len(sentences), batch_size)\n",
    "    if show_progress_bar:\n",
    "        iterator = tqdm(iterator, desc=\"Batches\")\n",
    "\n",
    "    tok_cnt = 0\n",
    "\n",
    "    for batch_idx in iterator:\n",
    "        batch_tokens = []\n",
    "        batch_masks = []\n",
    "        batch_weights = []\n",
    "\n",
    "        batch_start = batch_idx\n",
    "        batch_end = min(batch_start + batch_size, len(sentences))\n",
    "        #print (\"Batch End\", batch_end)\n",
    "        longest_seq = 0\n",
    "\n",
    "        # Get the maximal token length\n",
    "        for idx in length_sorted_idx[batch_start: batch_end]:\n",
    "            sentence = sentences[idx]\n",
    "            try:\n",
    "                tempTK = tokenizer.encode(sentence)\n",
    "                tempLen = len(tempTK)\n",
    "            except:\n",
    "                tempLen = maxLen\n",
    "            longest_seq = np.max([longest_seq, tempLen])\n",
    "\n",
    "        maxLen4Batch = np.min([longest_seq, maxLen])\n",
    "        for idx in length_sorted_idx[batch_start: batch_end]:\n",
    "            sentence = sentences[idx]\n",
    "            tokens = tokenizer.tokenize(sentence, do_lower_case=False)\n",
    "            tokens = ['[CLS]'] + tokens[:maxLen4Batch-2] + ['[SEP]']\n",
    "            padded_tokens = tokens + [\n",
    "                '[PAD]' for _ in range(maxLen4Batch - len(tokens))]\n",
    "            # Get attention masks for each sentence\n",
    "            specialToks = ['[CLS]', '[SEP]', '[PAD]']\n",
    "            attn_mask = [\n",
    "                1 if token not in specialToks\n",
    "                else 0\n",
    "                for token in padded_tokens]\n",
    "\n",
    "            attn_weights = [0 for _ in range(len(attn_mask))]\n",
    "\n",
    "            for i, tok in enumerate(padded_tokens):\n",
    "                if attn_mask[i] == 1:\n",
    "                    attn_weights[i] = idfDict[tok]\n",
    "\n",
    "                    #print (padded_tokens)\n",
    "            #print (attn_weights)\n",
    "\n",
    "            sumWeights = np.sum(attn_weights)\n",
    "            if sumWeights == 0:\n",
    "                attn_weights = [1 for _ in range(len(attn_weights))]\n",
    "            attn_weights = [x/sumWeights for x in attn_weights]\n",
    "\n",
    "            # Get BERT vocabulary index for each token\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "            batch_tokens.append(token_ids)\n",
    "            batch_masks.append(attn_mask)\n",
    "            batch_weights.append(attn_weights)\n",
    "\n",
    "        tok_cnt += len(batch_tokens)\n",
    "        batch_tokens = torch.tensor(batch_tokens).to(device)\n",
    "        batch_masks = torch.tensor(batch_masks).to(device)\n",
    "\n",
    "        if len(batch_weights) == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            #Feed to pretrained BERT model for embedding\n",
    "            embeddings = torch.nn.parallel.data_parallel(\n",
    "                model, batch_tokens,\n",
    "                module_kwargs={'attention_mask': batch_masks})[0] \\\n",
    "                if is_parallel \\\n",
    "                else model.forward(\n",
    "                batch_tokens, attention_mask=batch_masks)[0]\n",
    "\n",
    "            # Regular Avg: input torch tensors; output list of vectors\n",
    "            embeddingsM = aggHFTembByMaskV2(embeddings, batch_masks)\n",
    "\n",
    "            # Weighted Avg: input torch tensors; output list of vectors\n",
    "            embeddingsW = aggHFTembByIDF(embeddings, batch_weights)\n",
    "\n",
    "            #print(batch_tokens.size, len(embeddingsM))\n",
    "            all_embeddingsW.extend(embeddingsW)\n",
    "            all_embeddingsM.extend(embeddingsM)\n",
    "        \n",
    "        #print (tok_cnt, len(all_embeddingsW))\n",
    "\n",
    "    print (\"Len all_embeddingsW = {}, Len all_embeddingsM  = {}, Len reverting_order = {}\"\n",
    "           .format(len(all_embeddingsW), len(all_embeddingsM), len(length_sorted_idx)))\n",
    "    reverting_order = np.argsort(length_sorted_idx)\n",
    "    all_embeddingsW = [all_embeddingsW[idx] for idx in reverting_order]\n",
    "    all_embeddingsM = [all_embeddingsM[idx] for idx in reverting_order]\n",
    "\n",
    "    return all_embeddingsM, all_embeddingsW\n",
    "\n",
    "def UnitVect(vect):\n",
    "    \"\"\"\n",
    "    Normalize to Unit Vector\n",
    "    :param vect: The vector to normalize\n",
    "    :return: normalized vector with length 1\n",
    "    \"\"\"\n",
    "    #\n",
    "    mag = np.linalg.norm(vect)\n",
    "\n",
    "    # Make sure we don't divide by zero!\n",
    "    if mag > 0:\n",
    "        return vect/mag\n",
    "    elif np.sum(vect) == 0:\n",
    "        return vect\n",
    "    elif math.isnan(mag):\n",
    "        #print(vect)\n",
    "        return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train he LGB model\n",
    "def trainLGB(df, xCol=\"norm_embs_mean\", yCol=\"labels\"):\n",
    "    RANDOM_STATE = 99\n",
    "    learning_rate = 0.01\n",
    "    num_leaves = 15\n",
    "    min_data_in_leaf = 200\n",
    "    feature_fraction = 0.6\n",
    "    num_boost_round = 2000\n",
    "    max_depth = 9\n",
    "    early_stopping_rounds = 20\n",
    "    \n",
    "    params = {\n",
    "          \"objective\" : \"multiclass\",\n",
    "          \"num_class\" : 16,\n",
    "          \"num_leaves\" : num_leaves,\n",
    "          \"max_depth\": max_depth,\n",
    "          \"learning_rate\" : learning_rate,\n",
    "          \"bagging_fraction\" : 0.9,  # subsample\n",
    "          \"feature_fraction\" : 0.8,  # colsample_bytree\n",
    "          \"bagging_freq\" : 3,        # subsample_freq\n",
    "          \"bagging_seed\" : 2021,\n",
    "          \"verbosity\" : -1 }\n",
    "    \n",
    "    X = featureDFedt[\"norm_embs_idfw\"].tolist()\n",
    "    l = featureDFedt[\"labels\"].tolist()\n",
    "    X_train, X_validate, label_train, label_validate = model_selection.train_test_split(X, l, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    dtrain = lgbm.Dataset(np.array([list(v) for v in X_train]), label_train)\n",
    "    dvalid = lgbm.Dataset(np.array([list(v) for v in X_validate]), label_validate, reference=dtrain)\n",
    "    \n",
    "    lgbM = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, verbose_eval=100,\n",
    "                            early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiki Movie Plot dataset from \"https://www.kaggle.com/jrobischon/wikipedia-movie-plots\"\n",
    "# Load dataframe\n",
    "fPath = './data/wiki_movie_plots_deduped.csv copy.zip'\n",
    "df = pd.read_csv(fPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "movies_df = processWikiPlotData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat inputs for pre-trained BERT encoder\n",
    "sentList, chunks, labels = genInputs4preBERT(movies_df, chunkSize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating sentence level embedding: two sets with diff agg methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load roBERTa tokens and model\n",
    "# Initial tokenizer and model\n",
    "modelType = 'roberta-base'\n",
    "print('Loading tokenizer')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(modelType)\n",
    "print('Loading pretrained model')\n",
    "model = RobertaModel.from_pretrained(modelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfDict = genIdfDict4BERTToks(tokenizer, sentList, saveObj = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Aggregated sentence embedding and save by chunk\n",
    "# Mean and Weighted Avg by IDF scores\n",
    "for i in range(len(chunks)):\n",
    "    fName = \"./embs/roBERTa_embs_part{}.pkl\".format(i+1)\n",
    "    curEmbs = encodeHFTwWeightByIDF(\n",
    "        tokenizer, model, chunks[i], idfDict, batch_size=64, maxLen=512,\n",
    "        show_progress_bar=None, device=torch.device('cpu'),\n",
    "        is_parallel=False)\n",
    "    print (\"Dump part {} to {}\".format(i+1, fName))\n",
    "    with open(fName, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            curEmbs,\n",
    "            f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding and merge to labels\n",
    "rawEmbsMean = []\n",
    "rawEmbsIdfW = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    fName = \"./embs/roBERTa_embs_part{}.pkl\".format(i+1)\n",
    "\n",
    "    print (\"Load part {} from {}\".format(i+1, fName))\n",
    "    with open(fName, 'rb') as f:\n",
    "        currEmbs = pickle.load(f)\n",
    "        \n",
    "    rawEmbsMean.extend(currEmbs[0])\n",
    "    rawEmbsIdfW.extend(currEmbs[1])\n",
    "    \n",
    "# Check sizes\n",
    "print (len(rawEmbsMean), len(rawEmbsIdfW), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DF and normalize the embeddings\n",
    "cols = [\"raw_embs_mean\", \"raw_embs_idfw\", \"labels\"]\n",
    "embDicts = {}\n",
    "for i, col in enumerate(cols):\n",
    "    embDicts[col] = [rawEmbsMean, rawEmbsIdfW, labels][i]\n",
    "featureDF = pd.DataFrame(embDicts)\n",
    "featureDF[\"norm_embs_mean\"] = featureDF[\"raw_embs_mean\"].apply(UnitVect)\n",
    "featureDF[\"norm_embs_idfw\"] = featureDF[\"raw_embs_idfw\"].apply(UnitVect)\n",
    "\n",
    "# Drop rows with nan in case\n",
    "featureDFedt = featureDF.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embCols = [\"norm_embs_mean\"]\n",
    "lgbmModels = [\"norm_embs_idfw\"]\n",
    "for col in embCols:\n",
    "    currModel = trainLGB(df, xCol=col, yCol=\"labels\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
